{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc223e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "##エスペラント文の文字形式の変換関数\n",
    "esperanto_to_x = { \"ĉ\": \"cx\", \"ĝ\": \"gx\", \"ĥ\": \"hx\", \"ĵ\": \"jx\", \"ŝ\": \"sx\", \"ŭ\": \"ux\",\n",
    "                   \"Ĉ\": \"Cx\", \"Ĝ\": \"Gx\", \"Ĥ\": \"Hx\", \"Ĵ\": \"Jx\", \"Ŝ\": \"Sx\", \"Ŭ\": \"Ux\",\n",
    "                   \"c^\": \"cx\", \"g^\": \"gx\", \"h^\": \"hx\", \"j^\": \"jx\", \"s^\": \"sx\", \"u^\": \"ux\",\n",
    "                    \"C^\": \"Cx\", \"G^\": \"Gx\", \"H^\": \"Hx\", \"J^\": \"Jx\", \"S^\": \"Sx\", \"U^\": \"Ux\"}\n",
    "x_to_jijofu={'cx': 'ĉ', 'gx': 'ĝ', 'hx': 'ĥ', 'jx': 'ĵ', 'sx': 'ŝ', 'ux': 'ŭ', 'Cx': 'Ĉ',\n",
    "             'Gx': 'Ĝ', 'Hx': 'Ĥ', 'Jx': 'Ĵ', 'Sx': 'Ŝ', 'Ux': 'Ŭ'}\n",
    "x_to_hat={'cx': 'c^', 'gx': 'g^', 'hx': 'h^', 'jx': 'j^', 'sx': 's^', 'ux': 'u^', 'Cx': 'C^',\n",
    "          'Gx': 'G^', 'Hx': 'H^', 'Jx': 'J^', 'Sx': 'S^', 'Ux': 'U^'}\n",
    "\n",
    "def replace_esperanto_chars(text,letter_dictionary):\n",
    "    for esperanto_char, x_char in letter_dictionary.items():\n",
    "        text = text.replace(esperanto_char, x_char)\n",
    "    return text\n",
    "#テスト用のエスペラント文\n",
    "# text = \"Ĝis revido! Mia nomo estas Ĵoĥano. Ĉu vi ŝatas ĥorojn? -Ne, mi s^tas felic^on. C^ S^ H^ c^ s^ h^  Ĉ Ĝ  Gxis revido! Mia nomo estas Jxohxano. Cxu vi sxatas hxorojn? -Ne, mi sxtas felicxon. Cx Sx Hx cx sx hx  Cx Gx\"\n",
    "#エスペラント文の文字形式の変換\n",
    "# replaced_text = replace_esperanto_chars(text,esperanto_to_x)\n",
    "# replaced_text =replace_esperanto_chars(text,x_to_jijofu)\n",
    "# replaced_text =replace_esperanto_chars(text,x_to_hat)\n",
    "# print(\"元のテキスト:\", text)\n",
    "# print(\"置換後のテキスト:\", replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "052b8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "##まず、\"世界语全部单词_大约44100个(原pejvo.txt).txt\"を小文字、X形式に変換する。 (変換コード上では小文字、X形式で統一。)\n",
    "with open(\"世界语全部单词_大约44100个(原pejvo.txt)_original2024620_utf8_2列目以降修正_2通りの分解修正__更に修正.txt\", 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    converted_text=replace_esperanto_chars(text,esperanto_to_x)#エスペラントの特殊文字を対応するx形式に変換。\n",
    "    converted_text=converted_text.lower()#小文字に変換。\n",
    "\n",
    "#結果を\"tmp.txt\"に書き出す。\n",
    "with open('tmp.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(converted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "166c5383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "def contains_digit(s):#対象の文字列sに数字となりうる文字列(数字)が含まれるかどうかを確認する関数\n",
    "    return any(char.isdigit() for char in s)\n",
    "\n",
    "result=[]\n",
    "# \"tmp.txt\"(\"世界语全部单词_大约44100个(原pejvo.txt).txt\"を小文字、X形式に変換したもの)を開く。\n",
    "with open(\"tmp.txt\", 'r', encoding='utf-8') as file:\n",
    "    # \"tmp.txt\"の各行をループ。\n",
    "    for line in file:\n",
    "        # ':'が出てくるまでの部分を取り出す。\n",
    "        line=line.replace('-','/')##20240618追加\n",
    "        word = line.split(\":\")[0]\n",
    "        word = word.lstrip('/')##日本版,1列目だけ\n",
    "        # さらに取り出した部分を'-'、' '、','で分ける。\",\"もごくまれに存在する('tial')\n",
    "        parts = re.split('-| |,', word)\n",
    "        # 各部分をループし、単語の語尾の形式によって品詞分類しながら、その語尾をカットする。\n",
    "        for jj in range(len(parts)):\n",
    "            if jj>=0:##1列目も2列目以降も\n",
    "                part=parts[jj]\n",
    "                if not (contains_digit(part) or len(part)<2):##2文字以上\n",
    "                    if \"/\" in part:\n",
    "                        if part.endswith(('/o','/on','oj','/o!','ojn','on!')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('名詞')\n",
    "                        elif part.endswith(('/a','/aj','/an','/an!')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('形容詞')\n",
    "                        elif part.endswith(('/e','/e!')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('副詞')\n",
    "                        elif part.endswith(('/e/n','/e/n!')):##'/e/n'は後で気をつける\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-2])]\n",
    "                            AA.append('副詞')    \n",
    "                        elif part.endswith(('/i','/u','/u!')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('動詞')\n",
    "                        elif part.endswith(('/n')):\n",
    "                            AA=[\"/\".join(part.split(\"/\")[:-1])]\n",
    "                            AA.append('n語')            \n",
    "                    else:\n",
    "                        AA=[part,\"無詞\"]\n",
    "                    result.append(AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e262e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "##すべての単語の語尾が正しくカットされているかどうかチェックする。 ここまで情報は何一つ減っていないはず。\n",
    "with open(\"检查世界语所有单词的结尾是否被正确切除(result).txt\",\"w\",encoding='utf-8') as g:\n",
    "    for hh in result:\n",
    "        if len(hh)==2:##不要な条件と思われる。\n",
    "            g.write(hh[0]+','+hh[1]+\"\\n\")\n",
    "            # g.write(replace_esperanto_chars(hh[0],x_to_jijofu)+\"##\"+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c81690c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11141"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##上の作業で抽出した、'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換するための、漢字置換リストを作成していく。\n",
    "##\"'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換し終えたリスト\"こそが最終的な漢字置換リストの大元になる。\n",
    "##'既に完全に語根分解された状態の単語'が対象であれば、文字数の多い語根順に漢字置換するだけで完璧な精度の漢字置換ができる!\n",
    "##ただし、その完璧な漢字置換のためにはあらかじめ\"世界语全部单词_大约44100个(原pejvo.txt).txt\"から\"从世界语全部单词_大约44100个(原pejvo.txt).txt中提取并输出世界语所有词根_大约11360个.txt_带中文日文注释.ipynb\"を用いてエスペラントの全語根を抽出しておく必要がある。\n",
    "\n",
    "replacements_dict={}##一旦辞書型を使う。(後で内容(value)を更新するため)\n",
    "with open(\"世界语所有词根_大约11222个_20240621.txt\", 'r', encoding='utf-8') as file:\n",
    "# with open(\"世界语所有词根_大约11289个_20240619.txt\", 'r', encoding='utf-8') as file:\n",
    "    ##\"世界语所有词根_大约11360个.txt\"は\"世界语全部单词_大约44100个(原pejvo.txt).txt\"から\"从世界语全部单词_大约44100个(原pejvo.txt).txt中提取并输出世界语所有词根_大约11360个.txt_带中文日文注释.ipynb\"を用いて抽出したエスペラントの全語根である。\n",
    "    roots = file.readlines()\n",
    "    for root in roots:\n",
    "        root = root.strip()\n",
    "        if not root.isdigit():##混入していた数字の'10'と'7'を削除\n",
    "            replacements_dict[root]=[root,len(root)]##各エスペラント語根に対する'置換後の単語'(この作業では元の置換対象の語根のまま)と、その置換順序として、'置換対象の語根の文字数'を設定。　置換順序の数字が大きい('置換対象の語根の文字数が多い')ほど、先に置換される仕組みにする。\n",
    "len(replacements_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ff9000",
   "metadata": {},
   "outputs": [],
   "source": [
    "##上の作業に引き続き、\"'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換するための、漢字置換リスト\"を作成していく。　\n",
    "##ここでは漢字置換リストに自分で作成したエスペラント語根の漢字化リストを反映させる。\n",
    "\n",
    "def conversion_format(hanzi, word, format_type):##変換形式を決める。\n",
    "    if format_type == 'HTML Format':\n",
    "        return '<ruby>{}<rt>{}</rt></ruby>'.format(hanzi, word)\n",
    "    elif format_type == 'Parentheses Format':\n",
    "        return '{}({})'.format(hanzi, word)\n",
    "    elif format_type == 'Parentheses Format2':\n",
    "        return '{}({})'.format(word, hanzi)\n",
    "    elif format_type == 'Only Hanzi':\n",
    "        return '{}'.format(hanzi)\n",
    "    \n",
    "# format_type='Parentheses Format'\n",
    "# format_type='Parentheses Format2'\n",
    "format_type='HTML Format'\n",
    "\n",
    "\n",
    "# input_file=\"Ruby形式＿日本語.csv\"\n",
    "input_file=\"20240316世界语词根列表＿包含2个字符的世界语词根.csv\"\n",
    "\n",
    "# input_file=\"世界语汉字表格_20240312.csv\"\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        j = line.split(',')\n",
    "        if len(j)>=2:\n",
    "            word,hanzi=j[0],j[1]\n",
    "            if (hanzi!='') and (word!='') and ('#' not in word):\n",
    "                replacements_dict[word]=[conversion_format(hanzi, word, format_type),len(word)]#辞書式配列では要素(key)に対する値(value)を後から更新できることを利用している。\n",
    "# with open(\"replacements_dict.txt\", 'w', encoding='utf-8') as file:\n",
    "#     for old,new in replacements_dict.items():\n",
    "#         file.write(f'{old},{new[0]},{new[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80537419",
   "metadata": {},
   "outputs": [],
   "source": [
    "##'漢字置換リスト'を置換順序の数字の大きさ順にソート。\n",
    "pre_replacements=[]\n",
    "for old,new in replacements_dict.items():\n",
    "    pre_replacements.append((old,new[0],new[1]))\n",
    "pre_replacements3 = sorted(pre_replacements, key=lambda x: x[2], reverse=True)#x[2]がポイント\n",
    "# pre_replacements3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4ce9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##'漢字置換リスト'の各置換に対して'place holder'を追加し、'replacements'リストとして完成させた。\n",
    "##'place holder'法とは、既に置換を終えた文字列が後続の置換によって重複して置換されてしまうことを避けるために、その置換を終えた部分に一時的に無関係な文字列(place holder)を置いておいて、\n",
    "##全ての置換が終わった後に、改めてその'無関係な文字列(place holder)'から'目的の置換後文字列'に変換していく手法である。\n",
    "\n",
    "with open('No.10000_500000.txt', 'r', encoding='utf-8') as file:##漢字置換時に用いる\"place holder\"ファイルを予め読み込んでおく。\n",
    "    loaded_strings = [line.strip() for line in file]\n",
    "\n",
    "replacements=[]\n",
    "for kk in range(len(pre_replacements3)):\n",
    "    replacements.append([pre_replacements3[kk][0],pre_replacements3[kk][1],loaded_strings[kk]])\n",
    "\n",
    "\n",
    "##置換に用いる関数。正規表現、C++など様々な形式の置換を試したが、pythonで'place holder'を用いる形式の置換が、最も処理が高速であった。(しかも大変シンプルでわかりやすい。)\n",
    "def safe_replace(text, replacements):\n",
    "    valid_replacements = {}\n",
    "    # 置換対象(old)を'place holder'に一時的に置換\n",
    "    for old, new, placeholder in replacements:\n",
    "        if old in text:\n",
    "            text = text.replace(old, placeholder)\n",
    "            valid_replacements[placeholder] = new# 後で置換後の文字列(new)に置換し直す必要がある'place holder'を辞書(valid_replacements)に記録しておく。\n",
    "    #'place holder'を置換後の文字列(new)に置換)\n",
    "    for placeholder, new in valid_replacements.items():\n",
    "        text = text.replace(placeholder, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "#replacementsリスト(\"'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換するための、漢字置換リスト\"の完成版)の内容確認\n",
    "with open(\"replacements_list.txt\", 'w', encoding='utf-8') as file:\n",
    "    for old,new,priority in replacements:\n",
    "        file.write(f'{old},{new},{priority}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b206c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50678"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973d454",
   "metadata": {},
   "source": [
    "以下の3つはスキップしても良い。(時間がかかるので)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84a1e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "##'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を実際にreplacementsリスト(漢字置換リストの完成版)によって漢字置換。　\n",
    "##ここで作成される、\"漢字置換し終えたリスト(辞書型)\"(SS)が最終的な漢字置換リストの大元になる。\n",
    "##リストresultまでは情報の損失は殆どないはず。\n",
    "\n",
    "SS={}\n",
    "for j in result:##20秒ほどかかる。　先にリストの要素を全て結合して、一つの文字列にしてから漢字置換する方法を試しても(上述)、さほど高速化しなかった。\n",
    "    if len(j)==2:##(j[0]がエスペラント語根、j[1]が品詞。)\n",
    "        if len(j[0])>=2:##2文字以上のエスペラント語根のみが対象\n",
    "            if j[0] in SS:\n",
    "                if j[1] not in SS[j[0]][1]:\n",
    "                    SS[j[0]] = [SS[j[0]][0],SS[j[0]][1] + ', ' + j[1]]##複数品詞の追加\n",
    "            else:\n",
    "                SS[j[0]]=[safe_replace(j[0], replacements),j[1]]##辞書敷配列の追加法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "439b5949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abak: ['abak', '名詞']\n",
      "abat/ec: ['abat/<ruby>性<rt>ec</rt></ruby>', '名詞']\n",
      "abat/ej: ['abat/<ruby>场<rt>ej</rt></ruby>', '名詞']\n",
      "abat/in: ['abat/<ruby>女<rt>in</rt></ruby>', '名詞']\n",
      "abat: ['abat', '名詞']\n",
      "abdik: ['<ruby>退<rt>abdik</rt></ruby>', '動詞, 名詞']\n",
      "abdomen: ['abdomen', '名詞']\n",
      "abel: ['<ruby>蜜蜂<rt>abel</rt></ruby>', '形容詞, 名詞']\n",
      "miel: ['<ruby>蜜<rt>miel</rt></ruby>', '名詞, 形容詞']\n",
      "pik/il: ['<ruby>刺<rt>pik</rt></ruby>/<ruby>具<rt>il</rt></ruby>', '名詞']\n"
     ]
    }
   ],
   "source": [
    "# 上の作業で作成した辞書型リスト(SS)の最初から20個分を表示\n",
    "for key, value in dict(list(SS.items())[:10]).items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1ba14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SS.txt\", 'w', encoding='utf-8') as file:\n",
    "    for old,new in  SS.items():\n",
    "        file.write(f'{old},{new[0]},{new[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0c0c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abak: ['abak', '名詞']\n",
      "abat/ec: ['abat/<ruby>性<rt>ec</rt></ruby>', '名詞']\n",
      "abat/ej: ['abat/<ruby>场<rt>ej</rt></ruby>', '名詞']\n",
      "abat/in: ['abat/<ruby>女<rt>in</rt></ruby>', '名詞']\n",
      "abat: ['abat', '名詞']\n",
      "abdik: ['<ruby>退<rt>abdik</rt></ruby>', '動詞, 名詞']\n",
      "abdomen: ['abdomen', '名詞']\n",
      "abel: ['<ruby>蜜蜂<rt>abel</rt></ruby>', '形容詞, 名詞']\n",
      "miel: ['<ruby>蜜<rt>miel</rt></ruby>', '名詞, 形容詞']\n",
      "pik/il: ['<ruby>刺<rt>pik</rt></ruby>/<ruby>具<rt>il</rt></ruby>', '名詞']\n"
     ]
    }
   ],
   "source": [
    "# # テキストファイルから辞書を再構築する\n",
    "# テキストファイルから辞書を再構築する   \n",
    "SS = {}\n",
    "with open(\"SS.txt\", 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 各行をコンマで分割\n",
    "        parts = line.strip().split(',')\n",
    "        if len(parts)>=3:\n",
    "            # parts[2:] のリストをカンマ区切りの文字列に変換\n",
    "            parts_of_speech = ', '.join(part.strip() for part in parts[2:])\n",
    "            # 分割したデータを辞書に追加（リストとして保存）\n",
    "            SS[parts[0]] = [parts[1], parts_of_speech]##:がないと品詞が一つになってしまう。\n",
    "# # 再構築された辞書の内容を表示\n",
    "# for key, value in dict(list(reconstructed_dict.items())[:5]).items():\n",
    "#     print(f\"{key}: {value}\")\n",
    "\n",
    "# 上の作業で作成した辞書型リスト(SS)の最初から20個分を表示\n",
    "for key, value in dict(list(SS.items())[:10]).items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c50173c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # スラッシュを取り除いたキーでデータを整理するための辞書\n",
    "# normalized_keys = {}\n",
    "\n",
    "# # 各キーからスラッシュを取り除き、既存のキーとして整理\n",
    "# for old, value in SS.items():\n",
    "#     # スラッシュを取り除く\n",
    "#     normalized_key = old.replace('/', '')\n",
    "\n",
    "#     # 辞書に追加\n",
    "#     if normalized_key not in normalized_keys:\n",
    "#         normalized_keys[normalized_key] = []\n",
    "#     normalized_keys[normalized_key].append((old, value))\n",
    "\n",
    "# # 抜き出すためのファイル出力\n",
    "# with open(\"SS_chofuku.txt\", 'w', encoding='utf-8') as file:\n",
    "#     for key, entries in normalized_keys.items():\n",
    "#         # 同じ語根を持つ要素が複数ある場合のみファイルに書き込む\n",
    "#         if len(entries) > 1:\n",
    "#             for old, value in entries:\n",
    "#                 file.write(f'{old},{value[0]},{value[1]}\\n')\n",
    "\n",
    "\n",
    "# # 各語根ごとに最長のキーを保持する辞書\n",
    "# max_length_keys = {}\n",
    "\n",
    "# # スラッシュを取り除いた語根をキーとして、最長のキーと値を保存\n",
    "# for old, value in SS.items():\n",
    "#     # スラッシュを取り除く\n",
    "#     normalized_key = old.replace('/', '')\n",
    "#     # 辞書にこの語根が存在するか、存在する場合は現在のキーと比較\n",
    "#     if normalized_key not in max_length_keys or len(max_length_keys[normalized_key][0]) < len(old):\n",
    "#         max_length_keys[normalized_key] = (old, value)\n",
    "\n",
    "# # 最終的な辞書を作成\n",
    "# SS_ = {k: v for _, (k, v) in max_length_keys.items()}\n",
    "\n",
    "# with open(\"SS.txt\", 'w', encoding='utf-8') as file:\n",
    "#     for old,new in  SS.items():\n",
    "#         file.write(f'{old},{new[0]},{new[1]}\\n')\n",
    "    \n",
    "# with open(\"SS_.txt\", 'w', encoding='utf-8') as file:\n",
    "#     for old,new in  SS_.items():\n",
    "#         file.write(f'{old},{new[0]},{new[1]}\\n')\n",
    "# len(SS),len(SS_)\n",
    "# 結果を出力\n",
    "# print(SS_)\n",
    "###品詞によって分けることは可能か？20240616\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f15b8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32045"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "968bc0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ここから、\"'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換し終えたリスト(辞書型)\"(SS)を最終的な漢字置換リストに成形していく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f1bbd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "##更改替换方式(关于如何更改汉字转换,请编辑第一个csv文件)  (置換の仕方の変更(漢字変換の仕方の変更については最初のcsvファイルを編集する))\n",
    "# never_used_as_roots_only=[\" vin \",\" lin \",\" min \",\" amas \"]\n",
    "# for i in never_used_as_roots_only:\n",
    "#     SS[i]=[i,\"無詞\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cc487f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ここで、2通りの語根分解を消去していると考えられるが、品詞別に語根分解の方法を残すべきではないのか→日本語版の全単語語根分解リストについては解決。\n",
    "\n",
    "## SS→QQ  (\"'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換し終えたリスト\"(SS)を最終的な漢字置換リストに成形していく。)\n",
    "##SSの'置換対象の単語'、'漢字置換後の単語'から\"/\"を抜く(html形式にしたい場合、\"</rt></ruby>\"は\"/\"を含むので要注意！)。\n",
    "##新たに置換優先順位を表す数字を追加し(漢字化する単語は'文字数×10000'、漢字化しない単語は'文字数×10000-2500')、辞書式配列QQとして保存。\n",
    "QQ={}\n",
    "for i,j in SS.items():##(iが置換対象の単語、j[0]が漢字置換後の単語、j[1]が品詞。)\n",
    "    if i==j[0]:##漢字化しない単語\n",
    "        QQ[i.replace('/', '')]=[j[0].replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),j[1],len(i.replace('/', ''))*10000-3000]##漢字化しない単語は優先順位を下げる\n",
    "    else:\n",
    "        QQ[i.replace('/', '')]=[j[0].replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),j[1],len(i.replace('/', ''))*10000]\n",
    "\n",
    "# QQ\n",
    "with open(\"QQ.txt\", 'w', encoding='utf-8') as file:\n",
    "    for old,new in  QQ.items():\n",
    "        file.write(f'{old},{new[0]},{new[1]},{new[2]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88cab41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 上の作業で作成した辞書型リスト(QQ)の最初から10個分を表示\n",
    "# for key, value in dict(list(QQ.items())[:10]).items():\n",
    "#     print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73637b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 基本的には動詞に対してのみ活用語尾を追加し、置換対象の単語の文字数を増やす(置換の優先順位を上げる。)\n",
    "\n",
    "verb_suffix_2l={'as':'as', 'is':'is', 'os':'os', 'us':'us','at':'at','it':'it','ot':'ot', 'ad':'ad','igx':'igx','ig':'ig','ant':'ant','int':'int','ont':'ont'}\n",
    "##接頭辞接尾時の追加については、主に動詞が対象である。\n",
    "verb_suffix_2l_2={}\n",
    "for d1,d2 in verb_suffix_2l.items():\n",
    "    verb_suffix_2l_2[d1]=safe_replace(d2, replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76991011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'as': 'as',\n",
       " 'is': 'is',\n",
       " 'os': 'os',\n",
       " 'us': 'us',\n",
       " 'at': 'at',\n",
       " 'it': 'it',\n",
       " 'ot': 'ot',\n",
       " 'ad': 'ad',\n",
       " 'igx': '<ruby>成<rt>igx</rt></ruby>',\n",
       " 'ig': '<ruby>使<rt>ig</rt></ruby>',\n",
       " 'ant': 'ant',\n",
       " 'int': 'int',\n",
       " 'ont': 'ont'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_suffix_2l_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8afa98d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 []\n",
      "0 []\n",
      "62 [['dietan', '<ruby>食<rt>diet</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>食<rt>diet</rt></ruby>an'], ['afrikan', '<ruby>非<rt>afrik</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>非<rt>afrik</rt></ruby>an'], ['movadan', '<ruby>动<rt>mov</rt></ruby>ad<ruby>员<rt>an</rt></ruby>', '<ruby>动<rt>mov</rt></ruby>adan'], ['akcian', '<ruby>股<rt>akci</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>股<rt>akci</rt></ruby>an'], ['montaran', '<ruby>山<rt>mont</rt></ruby><ruby>群<rt>ar</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>山<rt>mont</rt></ruby><ruby>群<rt>ar</rt></ruby>an'], ['forten', '<ruby>离<rt>for</rt></ruby><ruby>持<rt>ten</rt></ruby>', '<ruby>强<rt>fort</rt></ruby>en'], ['amerikan', '<ruby>美<rt>amerik</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>美<rt>amerik</rt></ruby>an'], ['regnan', '<ruby>国<rt>regn</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>国<rt>regn</rt></ruby>an'], ['aparten', '<ruby>属<rt>aparten</rt></ruby>', '<ruby>独<rt>apart</rt></ruby>en'], ['dezertan', '<ruby>沙漠<rt>dezert</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>沙漠<rt>dezert</rt></ruby>an'], ['asocian', '<ruby>协会<rt>asoci</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>协会<rt>asoci</rt></ruby>an'], ['insulan', '<ruby>岛<rt>insul</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>岛<rt>insul</rt></ruby>an'], ['azian', '<ruby>亚<rt>azi</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>亚<rt>azi</rt></ruby>an'], ['sxtatan', '<ruby>国<rt>sxtat</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>国<rt>sxtat</rt></ruby>an'], ['doman', '<ruby>宅<rt>dom</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>宅<rt>dom</rt></ruby>an'], ['montan', '<ruby>山<rt>mont</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>山<rt>mont</rt></ruby>an'], ['familian', '<ruby>家<rt>famili</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>家<rt>famili</rt></ruby>an'], ['urban', '<ruby>城<rt>urb</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>城<rt>urb</rt></ruby>an'], ['inka', 'inka', '<ruby>墨<rt>ink</rt></ruby>a'], ['popolan', '<ruby>民<rt>popol</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>民<rt>popol</rt></ruby>an'], ['platen', 'platen', '<ruby>平<rt>plat</rt></ruby>en'], ['partian', '<ruby>派<rt>parti</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>派<rt>parti</rt></ruby>an'], ['reten', '<ruby>再<rt>re</rt></ruby><ruby>持<rt>ten</rt></ruby>', '<ruby>网<rt>ret</rt></ruby>en'], ['posten', '<ruby>岗<rt>posten</rt></ruby>', '<ruby>后<rt>post</rt></ruby>en'], ['pere', '<ruby>毁<rt>pere</rt></ruby>', '<ruby>凭<rt>per</rt></ruby>e'], ['lokan', '<ruby>位置<rt>lok</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>位置<rt>lok</rt></ruby>an'], ['sxipan', '<ruby>船<rt>sxip</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>船<rt>sxip</rt></ruby>an'], ['eklezian', '<ruby>教堂<rt>eklezi</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>教堂<rt>eklezi</rt></ruby>an'], ['landan', '<ruby>土<rt>land</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>土<rt>land</rt></ruby>an'], ['orientan', '<ruby>东<rt>orient</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>东<rt>orient</rt></ruby>an'], ['lernejan', '<ruby>学<rt>lern</rt></ruby><ruby>场<rt>ej</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>学<rt>lern</rt></ruby><ruby>场<rt>ej</rt></ruby>an'], ['enlandan', '<ruby>中<rt>en</rt></ruby><ruby>土<rt>land</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>中<rt>en</rt></ruby><ruby>土<rt>land</rt></ruby>an'], ['orden', 'orden', '<ruby>序<rt>ord</rt></ruby>en'], ['estraran', '<ruby>长<rt>estr</rt></ruby><ruby>群<rt>ar</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>长<rt>estr</rt></ruby><ruby>群<rt>ar</rt></ruby>an'], ['euxropan', '<ruby>欧<rt>euxrop</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>欧<rt>euxrop</rt></ruby>an'], ['polican', '<ruby>警<rt>polic</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>警<rt>polic</rt></ruby>an'], ['teren', '<ruby>土地<rt>teren</rt></ruby>', '<ruby>土<rt>ter</rt></ruby>en'], ['fero', 'fero', '<ruby>铁<rt>fer</rt></ruby>o'], ['socian', '<ruby>社<rt>soci</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>社<rt>soci</rt></ruby>an'], ['societan', '<ruby>社会<rt>societ</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>社会<rt>societ</rt></ruby>an'], ['grupan', '<ruby>群<rt>grup</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>群<rt>grup</rt></ruby>an'], ['havaj', 'havaj', '<ruby>有<rt>hav</rt></ruby>aj'], ['helen', 'helen', '<ruby>光<rt>hel</rt></ruby>en'], ['ligan', '<ruby>联<rt>lig</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>联<rt>lig</rt></ruby>an'], ['karen', 'karen', '<ruby>爱<rt>kar</rt></ruby>en'], ['nacian', '<ruby>民<rt>naci</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>民<rt>naci</rt></ruby>an'], ['koran', 'koran', '<ruby>心<rt>kor</rt></ruby>an'], ['kore', 'kore', '<ruby>心<rt>kor</rt></ruby>e'], ['religian', '<ruby>宗教<rt>religi</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>宗教<rt>religi</rt></ruby>an'], ['kuban', '<ruby>块<rt>kub</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>块<rt>kub</rt></ruby>an'], ['malaj', 'malaj', '<ruby>非<rt>mal</rt></ruby>aj'], ['male', 'male', '<ruby>非<rt>mal</rt></ruby>e'], ['nordan', '<ruby>北<rt>nord</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>北<rt>nord</rt></ruby>an'], ['paran', 'paran', '<ruby>对<rt>par</rt></ruby>an'], ['samo', 'samo', '<ruby>同<rt>sam</rt></ruby>o'], ['satan', 'satan', '<ruby>饱<rt>sat</rt></ruby>an'], ['savoj', 'savoj', '<ruby>救<rt>sav</rt></ruby>oj'], ['senatan', '<ruby>参<rt>senat</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>参<rt>senat</rt></ruby>an'], ['solen', '<ruby>威<rt>solen</rt></ruby>', '<ruby>独<rt>sol</rt></ruby>en'], ['sudan', 'sudan', '<ruby>南<rt>sud</rt></ruby>an'], ['veto', 'veto', '<ruby>赌<rt>vet</rt></ruby>o'], ['vilagxan', '<ruby>村<rt>vilagx</rt></ruby><ruby>员<rt>an</rt></ruby>', '<ruby>村<rt>vilagx</rt></ruby>an']]\n",
      "0 []\n",
      "3 [['amas', '<ruby>群<rt>amas</rt></ruby>', '<ruby>爱<rt>am</rt></ruby>as'], ['iris', 'iris', '<ruby>去<rt>ir</rt></ruby>is'], ['irit', 'irit', '<ruby>去<rt>ir</rt></ruby>it']]\n",
      "33 [['regulus', 'regulus', '<ruby>规<rt>regul</rt></ruby>us'], ['akirant', 'akirant', '<ruby>获<rt>akir</rt></ruby>ant'], ['radius', 'radius', '<ruby>射<rt>radi</rt></ruby>us'], ['premis', 'premis', '<ruby>压<rt>prem</rt></ruby>is'], ['format', 'format', '<ruby>型<rt>form</rt></ruby>at'], ['markot', 'markot', '<ruby>标<rt>mark</rt></ruby>ot'], ['nomad', 'nomad', '<ruby>名<rt>nom</rt></ruby>ad'], ['kolorad', 'kolorad', '<ruby>色<rt>kolor</rt></ruby>ad'], ['diskont', '<ruby>折扣<rt>diskont</rt></ruby>', '<ruby>盘<rt>disk</rt></ruby>ont'], ['endos', 'endos', '<ruby>必<rt>end</rt></ruby>os'], ['esperant', '<ruby>语<rt>esperant</rt></ruby>', '<ruby>希望<rt>esper</rt></ruby>ant'], ['forkant', '<ruby>离<rt>for</rt></ruby><ruby>歌<rt>kant</rt></ruby>', '<ruby>叉<rt>fork</rt></ruby>ant'], ['gravit', 'gravit', '<ruby>重<rt>grav</rt></ruby>it'], ['konus', 'konus', '<ruby>识<rt>kon</rt></ruby>us'], ['salat', '<ruby>沙<rt>salat</rt></ruby>', '<ruby>盐<rt>sal</rt></ruby>at'], ['lotus', '<ruby>莲<rt>lotus</rt></ruby>', '<ruby>彩<rt>lot</rt></ruby>us'], ['malvolont', '<ruby>非<rt>mal</rt></ruby><ruby>愿<rt>volont</rt></ruby>', '<ruby>非<rt>mal</rt></ruby><ruby>愿<rt>vol</rt></ruby>ont'], ['mankis', '<ruby>手<rt>man</rt></ruby><ruby>吻<rt>kis</rt></ruby>', '<ruby>缺<rt>mank</rt></ruby>is'], ['minus', '<ruby>减<rt>minus</rt></ruby>', '<ruby>矿<rt>min</rt></ruby>us'], ['patos', 'patos', '<ruby>锅<rt>pat</rt></ruby>os'], ['predikat', '<ruby>述<rt>predikat</rt></ruby>', '<ruby>宣<rt>predik</rt></ruby>at'], ['rabat', '<ruby>折扣<rt>rabat</rt></ruby>', '<ruby>抢<rt>rab</rt></ruby>at'], ['rabot', '<ruby>刨<rt>rabot</rt></ruby>', '<ruby>抢<rt>rab</rt></ruby>ot'], ['remont', 'remont', '<ruby>船<rt>rem</rt></ruby>ont'], ['satirus', 'satirus', '<ruby>讽<rt>satir</rt></ruby>us'], ['sendat', '<ruby>无<rt>sen</rt></ruby><ruby>日<rt>dat</rt></ruby>', '<ruby>送<rt>send</rt></ruby>at'], ['sendot', '<ruby>无<rt>sen</rt></ruby><ruby>资<rt>dot</rt></ruby>', '<ruby>送<rt>send</rt></ruby>ot'], ['spirit', '<ruby>灵魂<rt>spirit</rt></ruby>', '<ruby>气<rt>spir</rt></ruby>it'], ['spirant', 'spirant', '<ruby>气<rt>spir</rt></ruby>ant'], ['tenis', 'tenis', '<ruby>持<rt>ten</rt></ruby>is'], ['traktat', '<ruby>条约<rt>traktat</rt></ruby>', '<ruby>处理<rt>trakt</rt></ruby>at'], ['trikot', '<ruby>针织<rt>trikot</rt></ruby>', '<ruby>织<rt>trik</rt></ruby>ot'], ['volont', '<ruby>愿<rt>volont</rt></ruby>', '<ruby>愿<rt>vol</rt></ruby>ont']]\n",
      "6 [['arogant', '<ruby>傲<rt>arogant</rt></ruby>', 'arogant'], ['brokant', '<ruby>古董<rt>brokant</rt></ruby>', 'brokant'], ['ferlad', '<ruby>铁<rt>fer</rt></ruby><ruby>板<rt>lad</rt></ruby>', 'ferlad'], ['revizit', '<ruby>再<rt>re</rt></ruby><ruby>参<rt>vizit</rt></ruby>', 'revizit'], ['trilit', 'tri<ruby>床<rt>lit</rt></ruby>', 'trilit'], ['vizit', '<ruby>参<rt>vizit</rt></ruby>', 'vizit']]\n",
      "0 []\n"
     ]
    }
   ],
   "source": [
    "###一番の工夫ポイント(如何にして置換の優先順位を定め、置換精度を向上させるか。)\n",
    "##基本は単語の文字数が多い順に置換していくことになるが、\n",
    "##例えば、\"置換対象の単語に接頭辞、接尾辞を追加し、単語の文字数を増やし、置換の優先順位を上げたものを、置換対象の単語として新たに追加する。\"などが、置換精度を上げる方策として考えられる。\n",
    "##しかし、いろいろ試した結果、動詞に対してのみ活用語尾を追加し、置換対象の単語の文字数を増やす(置換の優先順位を上げる。)のが、ベストに近いことがわかった。\n",
    "\n",
    "## SS→QQ→RR  (\"'単語の語尾だけをカットした、完全に語根分解された状態の全単語リスト'(result)を漢字置換し終えたリスト\"(SS)を最終的な漢字置換リストに成形していく。)\n",
    "### if not i+k in QQ:の働きを調べる。 \n",
    "### if not i+k in QQ:の働きを調べる。 \n",
    "UU_0=[]\n",
    "UU=[]\n",
    "VV=[]\n",
    "WW=[]\n",
    "\n",
    "UU_2=[]\n",
    "VV_2=[]\n",
    "WW_2=[]\n",
    "\n",
    "XX=[]\n",
    "\n",
    "RR={}\n",
    "# 辞書をコピーする\n",
    "QQ_copy = QQ.copy()##これがあるので、2回繰り返しするときは数個前のセルに戻ってQQを作り直してからでないといけない。\n",
    "for i,j in QQ_copy.items():##j[0]:置換後の文字列　j[1]:品詞 j[2]:置換優先順位\n",
    "    if (j[1] == \"名詞\") and (len(i)<=6) and not(j[2]==60000 or j[2]==50000 or j[2]==40000 or j[2]==30000 or j[2]==20000):##名詞だけで、6文字以下で、漢字化しないやつ  ##置換ミスを防ぐための条件(20240614) altajo,aviso,malm,abes 固有名詞対策  意味ふりがなのときは再検討\n",
    "        for k in [\"o\"]:##4 ['buro', 'haloo', 'tauxro', 'unesko']\n",
    "            if not i+k in QQ_copy:##if not ありのままで良い。\n",
    "                RR[i+k]=[j[0]+k,j[2]+len(k)*10000-2000]#実質5000 #既存でないものは優先順位を大きく下げる→普通の品詞接尾辞が既存でないという言い方はおかしい気がしてきた。(20240612)\n",
    "            elif j[0]+k != QQ_copy[i+k][0]:##該当なし\n",
    "                UU_0.append([i+k,QQ_copy[i+k][0],j[0]+k])\n",
    "        QQ.pop(i, None)\n",
    "\n",
    "for i,j in QQ.items():##j[0]:置換後の文字列　j[1]:品詞 j[2]:置換優先順位\n",
    "    if j[2]==20000:##2文字で漢字化するやつ##len(i)<=2:#1文字は存在しないはずではある。\n",
    "        ##基本的に非動詞の2文字の語根単体を以て漢字置換することはない。　ただし、世界语全部单词_大约44100个(原pejvo.txt).txtに最初から含まれている2文字の語根は既に漢字化されており、実際の漢字置換にも反映されることになる。\n",
    "        ##2文字の語根でも、動詞については活用語尾を追加することで、自動的に+2文字以上できるので追加した。\n",
    "        if \"名詞\" in j[1]:\n",
    "            for k in [\"o\",\"on\",'oj']:##\"ojn\"は不要か\n",
    "                if not i+k in QQ:\n",
    "                    RR[' '+i+k]=[' '+j[0]+k,j[2]+(len(k)+1)*10000-5000]\n",
    "                else:\n",
    "                    UU.append([i+k,QQ[i+k][0],j[0]+k])\n",
    "        if \"形容詞\" in j[1]:\n",
    "            for k in [\"a\",\"aj\",'an']:##\"ajn\"は不要か  ##sia pian\n",
    "                # if not i+k in QQ:##if not なしのほうが良い\n",
    "                RR[' '+i+k]=[' '+j[0]+k,j[2]+(len(k)+1)*10000-5000]\n",
    "                # else:\n",
    "                #     UU.append(i+k)\n",
    "        if \"副詞\" in j[1]:\n",
    "            for k in [\"e\",'en']:##ege   エーゲ海を意味するegeoを元の辞書に追加\n",
    "                # if not i+k in QQ:##if not なしのほうが良い\n",
    "                RR[' '+i+k]=[' '+j[0]+k,j[2]+(len(k)+1)*10000-5000]\n",
    "                # else:\n",
    "                # UU.append(i+k)\n",
    "        if \"動詞\" in j[1]:\n",
    "            for k1,k2 in verb_suffix_2l_2.items():\n",
    "                if not i+k1 in QQ:\n",
    "                    RR[i+k1]=[j[0]+k2,j[2]+len(k1)*10000-3000]\n",
    "                elif j[0]+k2 != QQ[i+k1][0]:\n",
    "                    UU_2.append([i+k1,QQ[i+k1][0],j[0]+k2])##該当なし\n",
    "            for k in [\"u \",\"u!\",\"u?\",\"u.\",\"i \",\"i.\",\"i?\"]:##動詞の\"u\",\"i\"単体の接尾辞は後ろが空白と決まっているので、2文字分増やすことができる。\n",
    "                if not i+k in QQ:\n",
    "                    RR[i+k]=[j[0]+k,j[2]+len(k)*10000-3000]\n",
    "                elif j[0]+k != QQ[i+k][0]:\n",
    "                    UU_2.append([i+k,QQ[i+k][0],j[0]+k])##該当なし\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        RR[i]=[j[0],j[2]]##品詞情報はここで用いるためにあった。以後は不要なので省いていく。\n",
    "        if j[2]==60000 or j[2]==50000 or j[2]==40000 or j[2]==30000:##文字数が比較的少なく(<=5)、実際に漢字化するエスペラント語根(文字数×10000)のみを対象とする \n",
    "            if \"名詞\" in j[1]:##名詞については形容詞、副詞と違い、漢字化しないものにもoをつける。\n",
    "                for k in [\"o\",\"oj\"]:\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+len(k)*10000-3000]#既存でないものは優先順位を大きく下げる→普通の品詞接尾辞が既存でないという言い方はおかしい気がしてきた。(20240612)\n",
    "                    elif j[0]+k != QQ[i+k][0]:\n",
    "                        VV.append([i+k,QQ[i+k][0],j[0]+k])##該当なし\n",
    "            if \"形容詞\" in j[1]:\n",
    "                for k in [\"a\",\"aj\",'an']:\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+len(k)*10000-3000]\n",
    "                    elif j[0]+k != QQ[i+k][0]:\n",
    "                        VV.append([i+k,QQ[i+k][0],j[0]+k])##該当なし\n",
    "            if \"副詞\" in j[1]:\n",
    "                for k in [\"e\",'en']:\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+len(k)*10000-3000]\n",
    "                    elif j[0]+k != QQ[i+k][0]:\n",
    "                        VV.append([i+k,QQ[i+k][0],j[0]+k])##該当なし\n",
    "            if \"動詞\" in j[1]:\n",
    "                for k1,k2 in verb_suffix_2l_2.items():\n",
    "                    if not i+k1 in QQ:\n",
    "                        RR[i+k1]=[j[0]+k2,j[2]+len(k1)*10000-3000]\n",
    "                    elif j[0]+k2 != QQ[i+k1][0]:\n",
    "                        VV_2.append([i+k1,QQ[i+k1][0],j[0]+k2])##該当なし\n",
    "                for k in [\"u \",\"u!\",\"i \"]:##動詞の\"u\",\"i\"単体の接尾辞は後ろが空白と決まっているので、2文字分増やすことができる。\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+len(k)*10000-3000]\n",
    "                    elif j[0]+k != QQ[i+k][0]:\n",
    "                        VV_2.append([i+k,QQ[i+k][0],j[0]+k])##該当なし\n",
    "                        \n",
    "        elif len(i)>=3 and len(i)<=6:##3文字から6文字の語根で漢字化しないもの　　結局2文字の語根で漢字化しないものについては、完全に除外している。\n",
    "            if \"名詞\" in j[1]:##名詞については形容詞、副詞と違い、漢字化しないものにもoをつける。\n",
    "                for k in [\"o\"]:\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+len(k)*10000-5000]#実質3000#既存でないものは優先順位を大きく下げる→普通の品詞接尾辞が既存でないという言い方はおかしい気がしてきた。(20240612)\n",
    "                    elif j[0]+k != QQ[i+k][0]:\n",
    "                        WW.append([i+k,QQ[i+k][0],j[0]+k])##該当なし\n",
    "            if \"形容詞\" in j[1]:\n",
    "                for k in [\"a\"]:\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+len(k)*10000-5000]\n",
    "                    elif j[0]+k != QQ[i+k][0]:\n",
    "                        WW.append([i+k,QQ[i+k][0],j[0]+k])##該当なし\n",
    "            if \"副詞\" in j[1]:\n",
    "                for k in [\"e\"]:\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+len(k)*10000-5000]\n",
    "                    elif j[0]+k != QQ[i+k][0]:\n",
    "                        WW.append([i+k,QQ[i+k][0],j[0]+k])##該当なし\n",
    "            if \"動詞\" in j[1]:\n",
    "                for k1,k2 in verb_suffix_2l_2.items():\n",
    "                    if not i+k1 in QQ:\n",
    "                        RR[i+k1]=[j[0]+k2,j[2]+(len(k1)-1)*10000-5000]\n",
    "                    elif j[0]+k2 != QQ[i+k1][0]:\n",
    "                        WW_2.append([i+k1,QQ[i+k1][0],j[0]+k2])##該当なし\n",
    "                for k in [\"u \",\"u!\",\"i \"]:##動詞の\"u\",\"i\"単体の接尾辞は後ろが空白と決まっているので、2文字分増やすことができる。\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+(len(k)-1)*10000-5000]\n",
    "                    elif j[0]+k != QQ[i+k][0]:\n",
    "                        WW_2.append([i+k,QQ[i+k][0],j[0]+k])##該当なし\n",
    "        \n",
    "        elif  i in [\"regulus\",\"kolorad\",\"satirus\",\"spirant\",\"traktat\",\"akirant\",\"ordinat\"]:#7文字以上の例外処理\n",
    "            if \"名詞\" in j[1]:##名詞については形容詞、副詞と違い、漢字化しないものにもoをつける。\n",
    "                for k in [\"o\"]:\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+len(k)*10000-5000]#実質3000#既存でないものは優先順位を大きく下げる→普通の品詞接尾辞が既存でないという言い方はおかしい気がしてきた。(20240612)\n",
    "            if \"形容詞\" in j[1]:\n",
    "                for k in [\"a\"]:\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+len(k)*10000-5000]\n",
    "            if \"副詞\" in j[1]:\n",
    "                for k in [\"e\"]:\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+len(k)*10000-5000]\n",
    "            if \"動詞\" in j[1]:\n",
    "                for k1,k2 in verb_suffix_2l_2.items():\n",
    "                    if not i+k1 in QQ:\n",
    "                        RR[i+k1]=[j[0]+k2,j[2]+(len(k1)-1)*10000-5000]\n",
    "                for k in [\"u \",\"u!\",\"i \"]:##動詞の\"u\",\"i\"単体の接尾辞は後ろが空白と決まっているので、2文字分増やすことができる。\n",
    "                    if not i+k in QQ:\n",
    "                        RR[i+k]=[j[0]+k,j[2]+(len(k)-1)*10000-5000]\n",
    "\n",
    "print(len(UU_0),UU_0)\n",
    "print(len(UU),UU)\n",
    "print(len(VV),VV)\n",
    "print(len(WW),WW)\n",
    "\n",
    "print(len(UU_2),UU_2)\n",
    "print(len(VV_2),VV_2)\n",
    "print(len(WW_2),WW_2)\n",
    "print(len(XX),XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9837296",
   "metadata": {},
   "outputs": [],
   "source": [
    "##RRの編集(主に置換の優先順位の変更) ここでも置換の仕方の変更ができないことはないが、品詞の種類に応じて接尾辞や接頭辞を追加するところをスキップすることになってしまう。\n",
    "\n",
    "never_used_as_roots_only=[\"vin\",\"lin\",\"sin\",\"min\"]\n",
    "for i in never_used_as_roots_only:\n",
    "    RR[i]=[i,len(i)*10000+3000]##これらについては数字の大きさはそこまで重要ではない\n",
    "# QQ[i.replace('/', '')]=[j[0].replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),j[1],len(i.replace('/', ''))*10000-3000]##漢字化しない単語は優先順位を下げる\n",
    "# conversion_format(hanzi, word, format_type)\n",
    "\n",
    "\n",
    "# RR['amas']=['<ruby>爱<rt>am</rt></ruby>as',len('amas')*10000+2500]##漢字化しない語根単体については上記で、うまく処理できているはずだが、amasoは群oと漢字化するので。\n",
    "RR['farigx'][1]=len('farigx')*10000+27500##優先順位だけ変更\n",
    "\n",
    "x='mondo/n'\n",
    "RR[x.replace('/', '')]=[safe_replace(x,replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len(x.replace('/', ''))*10000+3000]\n",
    "\n",
    "##正しく語根分解・漢字変換してほしいやつ  anとenは念の為a/n/,e/n/としておく。ant,int,ontは大丈夫  空白を使うのは最終手段。  \",\"は絶対に使っては駄目\n",
    "y1=[['gvid/ant/o',73000],['am/as',33000],[\"kor/a/n\",43000],[\"ink/a\",33000],[\"post/e/n\",53000],[\"per/e\",33000],[\"fer/o\",33000],[\"kor/e\",33000],[\"mal/a/j\",43000],[\"mal/e\",33000],\n",
    "    [\"par/a/n\",33000],['sam/o',33000],['sat/a/n',33000],['sav/oj',43000],['sud/a/n',43000],['vet/o',33000],['ir/is',33000],['regul/us',63000],['akir/ant',63000],[\"prem/is\",53000],\n",
    "    [\"mark/ot\",53000],[\"kolor/ad\",63000],[\"lot/us\",43000],[\"mank/is\",53000],[\"pat/os\",43000],[\"rem/ont\",53000],[\"satir/us\",63000],[\"send/at\",53000],[\"send/ot\",53000]\n",
    "    ,[\"spir/ant\",63000],[\"ten/is \",53000],[\"trakt/at\",63000],[\"alt/e\",33000],[\"apog/e \",53000],[\"dom/e/n\",4300],[\"kaz/e/ \",33000],[\"dek/a/n\",43000]\n",
    "    ,[\"post/e/n\",53000],[\"posten/ul\",73000],[\"kalk/a/n \",53000],[\"faz/a/n \",43000],[\"hav/a/j\",43000],[\"sol/e \",33000],[\"lam/a\",33000],[\"nord/a/n \",63000]\n",
    "    ,[\"ref/oj \",53000],[\"ref/oj/n\",53000],[\"akir/ant\",63000],[\"ordin/at\",63000],[\"form/at\",53000],[\"kant/at\",53000],[\"end/os\",43000]\n",
    "    ,[\"konus \",53000],[\"lek/ant\",53000],[\"leg/at\",43000],[\"taks/us\",53000]]##[\"pi/a/n\",38000] anをa/n/で分けるのは正しくはないが。  havaj\n",
    "for i in y1:\n",
    "    RR[i[0].replace('/', '')]=[safe_replace(i[0],replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"), i[1]]\n",
    "\n",
    "##漢字変換してほしくないやつ\n",
    "y2=[['lian',43000]]\n",
    "for j in y2:\n",
    "    RR[j[0].replace('/', '')]=[conversion_format(j[0],j[0], format_type), j[1]]\n",
    "\n",
    "\n",
    "##以下は完全手作業\n",
    "RR['dat/um/i'.replace('/', '')]=[safe_replace('dat/um/i',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('dat/um/i'.replace('/', ''))*10000+3000]\n",
    "RR['dat/um/u'.replace('/', '')]=[safe_replace('dat/um/u',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('dat/um/u'.replace('/', ''))*10000+3000]\n",
    "RR['dat/um/u!'.replace('/', '')]=[safe_replace('dat/um/u!',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('dat/um/u!'.replace('/', ''))*10000+3000]\n",
    "#dat/um/u  dat/um/u!\n",
    "RR['tra/met/i'.replace('/', '')]=[safe_replace('tra/met/i',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('tra/met/i'.replace('/', ''))*10000+3000]\n",
    "RR['tra/met/u'.replace('/', '')]=[safe_replace('tra/met/u',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('tra/met/u'.replace('/', ''))*10000+3000]\n",
    "RR['tra/met/u!'.replace('/', '')]=[safe_replace('tra/met/u!',replacements).replace(\"</rt></ruby>\",\"%%%\").replace('/', '').replace(\"%%%\",\"</rt></ruby>\"),  len('tra/met/u!'.replace('/', ''))*10000+3000]\n",
    "\n",
    "# RR['trametu!']=['<ruby>通<rt>tra</rt></ruby><ruby>置<rt>met</rt></ruby>u!',len('trametu!')*10000+2500]\n",
    "#tra/met/u  tra/met/u!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b4b2752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<ruby>导<rt>gvid</rt></ruby>anto', 73000] ['<ruby>爱<rt>am</rt></ruby>as', 33000] ['<ruby>心<rt>kor</rt></ruby>an', 43000] ['<ruby>lian<rt>lian</rt></ruby>', 43000]\n",
      "['<ruby>墨<rt>ink</rt></ruby>a', 33000] ['<ruby>后<rt>post</rt></ruby>en', 53000] ['<ruby>凭<rt>per</rt></ruby>e', 33000] ['<ruby>铁<rt>fer</rt></ruby>o', 33000]\n",
      "['korano', 52000] ['<ruby>心<rt>kor</rt></ruby>e', 33000] ['<ruby>非<rt>mal</rt></ruby>aj', 43000] ['<ruby>非<rt>mal</rt></ruby>e', 33000] ['<ruby>对<rt>par</rt></ruby>an', 33000] ['<ruby>同<rt>sam</rt></ruby>o', 33000]\n",
      "['<ruby>饱<rt>sat</rt></ruby>an', 33000] ['<ruby>救<rt>sav</rt></ruby>oj', 43000] ['<ruby>南<rt>sud</rt></ruby>an', 43000] ['<ruby>赌<rt>vet</rt></ruby>o', 33000] ['<ruby>去<rt>ir</rt></ruby>is', 33000] ['<ruby>规<rt>regul</rt></ruby>us', 63000]\n",
      "['<ruby>获<rt>akir</rt></ruby>ant', 63000] ['<ruby>压<rt>prem</rt></ruby>is', 53000] ['<ruby>标<rt>mark</rt></ruby>ot', 53000] ['<ruby>色<rt>kolor</rt></ruby>ad', 63000] ['<ruby>去<rt>ir</rt></ruby>is', 33000] ['<ruby>规<rt>regul</rt></ruby>us', 63000]\n",
      "['<ruby>彩<rt>lot</rt></ruby>us', 43000] ['<ruby>压<rt>prem</rt></ruby>is', 53000] ['<ruby>标<rt>mark</rt></ruby>ot', 53000] ['<ruby>色<rt>kolor</rt></ruby>ad', 63000] ['<ruby>彩<rt>lot</rt></ruby>us', 43000] ['<ruby>缺<rt>mank</rt></ruby>is', 53000]\n",
      "['<ruby>锅<rt>pat</rt></ruby>os', 43000] ['<ruby>船<rt>rem</rt></ruby>ont', 53000] ['<ruby>讽<rt>satir</rt></ruby>us', 63000] ['<ruby>送<rt>send</rt></ruby>at', 53000] ['<ruby>送<rt>send</rt></ruby>ot', 53000] ['<ruby>气<rt>spir</rt></ruby>ant', 63000]\n",
      "['<ruby>持<rt>ten</rt></ruby>is ', 53000] ['<ruby>处理<rt>trakt</rt></ruby>at', 63000] ['<ruby>高<rt>alt</rt></ruby>e', 33000] ['<ruby>支持<rt>apog</rt></ruby>e ', 53000] ['<ruby>宅<rt>dom</rt></ruby>en', 4300] ['<ruby>情况<rt>kaz</rt></ruby>e ', 33000]\n",
      "['dekan', 43000] ['<ruby>后<rt>post</rt></ruby>en', 53000] ['<ruby>岗<rt>posten</rt></ruby><ruby>人<rt>ul</rt></ruby>', 73000] ['kalkan ', 53000] ['fazan ', 43000] ['<ruby>有<rt>hav</rt></ruby>aj', 43000]\n",
      "['<ruby>独<rt>sol</rt></ruby>e ', 33000] ['<ruby>瘸<rt>lam</rt></ruby>a', 33000] ['<ruby>非<rt>mal</rt></ruby>aj', 43000] ['<ruby>北<rt>nord</rt></ruby>an ', 63000] ['refoj ', 53000] ['refojn', 53000]\n",
      "['ordinat', 63000] ['<ruby>获<rt>akir</rt></ruby>ant', 63000] ['<ruby>型<rt>form</rt></ruby>at', 53000] ['<ruby>歌<rt>kant</rt></ruby>at', 53000] ['<ruby>必<rt>end</rt></ruby>os', 43000]\n",
      "['konus ', 53000] ['<ruby>尝<rt>lek</rt></ruby>ant', 53000] ['<ruby>阅<rt>leg</rt></ruby>at', 43000] ['<ruby>评<rt>taks</rt></ruby>us', 53000] ['reguluso', 72000] ['<ruby>必<rt>end</rt></ruby>os', 43000]\n",
      "['reguluso', 72000] ['akiranto', 72000] ['kolorado', 72000] ['satiruso', 72000] ['spiranto', 72000] ['<ruby>条约<rt>traktat</rt></ruby>o', 75000] ['<ruby>北<rt>nord</rt></ruby><ruby>员<rt>an</rt></ruby>o', 67000] ['akiranto', 72000] ['ordinato', 72000]\n"
     ]
    }
   ],
   "source": [
    "print(RR[\"gvidanto\"],RR[\"amas\"],RR[\"koran\"],RR[\"lian\"])\n",
    "print(RR[\"inka\"],RR[\"posten\"],RR[\"pere\"],RR[\"fero\"])\n",
    "print(RR[\"korano\"],RR[\"kore\"],RR[\"malaj\"],RR[\"male\"],RR[\"paran\"],RR[\"samo\"])\n",
    "print(RR[\"satan\"],RR[\"savoj\"],RR[\"sudan\"],RR[\"veto\"],RR[\"iris\"],RR[\"regulus\"])\n",
    "print(RR[\"akirant\"],RR[\"premis\"],RR[\"markot\"],RR[\"kolorad\"],RR[\"iris\"],RR[\"regulus\"])\n",
    "print(RR[\"lotus\"],RR[\"premis\"],RR[\"markot\"],RR[\"kolorad\"],RR[\"lotus\"],RR[\"mankis\"])\n",
    "print(RR[\"patos\"],RR[\"remont\"],RR[\"satirus\"],RR[\"sendat\"],RR[\"sendot\"],RR[\"spirant\"])\n",
    "print(RR[\"tenis \"],RR[\"traktat\"],RR[\"alte\"],RR[\"apoge \"],RR[\"domen\"],RR[\"kaze \"])\n",
    "print(RR[\"dekan\"],RR[\"posten\"],RR[\"postenul\"],RR[\"kalkan \"],RR[\"fazan \"],RR[\"havaj\"])\n",
    "print(RR[\"sole \"],RR[\"lama\"],RR[\"malaj\"],RR[\"nordan \"],RR[\"refoj \"],RR[\"refojn\"])\n",
    "print(RR[\"ordinat\"],RR[\"akirant\"],RR[\"format\"],RR[\"kantat\"],RR[\"endos\"])\n",
    "print(RR[\"konus \"],RR[\"lekant\"],RR[\"legat\"],RR[\"taksus\"],RR[\"reguluso\"],RR[\"endos\"])\n",
    "\n",
    "print(RR[\"reguluso\"],RR[\"akiranto\"],RR[\"kolorado\"],RR[\"satiruso\"],RR[\"spiranto\"]\n",
    "      ,RR[\"traktato\"],RR[\"nordano\"],RR[\"akiranto\"],RR[\"ordinato\"])\n",
    "# zzzz=[['regul/us',63000],['akir/ant',63000],[\"kolor/ad\",63000],[\"satir/us\",63000]\n",
    "#     ,[\"spir/ant\",63000],[\"trakt/at\",63000],[\"nord/a/n \",63000],[\"akir/ant\",63000],[\"ordin/at\",63000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fff68d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"RR.txt\", 'w', encoding='utf-8') as file:\n",
    "    for old,new in  RR.items():\n",
    "        file.write(f'{old},{new[0]},{new[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08c5cf97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<ruby>群<rt>amas</rt></ruby>',\n",
       " '<ruby>矿<rt>min</rt></ruby>',\n",
       " '<ruby>世界<rt>mond</rt></ruby>o/n',\n",
       " '<ruby>导<rt>gvid</rt></ruby>/ant/o')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##リストreplacementはコードの前半と後半で別物になるので注意が必要。\n",
    "safe_replace(\"amas\",replacements),safe_replace(\"min\",replacements),safe_replace(\"mondo/n\",replacements),safe_replace('gvid/ant/o',replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4011280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TT=[]\n",
    "for old,new in  RR.items():\n",
    "    if isinstance(new[1], int):\n",
    "        TT.append((old,new[0],new[1]))\n",
    "# print(len(TT),len(RR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "053fc408",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_replacements3= sorted(TT, key=lambda x: x[2], reverse=True)##(置換順序の数字の大きさ順にソート!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3ec2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pre_replacements3.txt\", 'w', encoding='utf-8') as file:\n",
    "    for old,new,priority in pre_replacements3:\n",
    "        file.write(f'{old},{new},{priority}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adcb681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##'エスペラント語根'、'置換漢字'、'place holder'の順に並べ、最終的な置換に用いる\"replacements\"リストを作成。\n",
    "replacements=[]\n",
    "for kk in range(len(pre_replacements3)):\n",
    "    if len(pre_replacements3[kk][0])>1:\n",
    "        replacements.append([pre_replacements3[kk][0],pre_replacements3[kk][1],loaded_strings[kk]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8511730a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90320"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e749f55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270960"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##'大文字'、'小文字'、'文頭だけ大文字'のケースに対応。\n",
    "replacements2=[]\n",
    "for old,new,place_holder in replacements:\n",
    "    replacements2.append((old,new,place_holder))\n",
    "    replacements2.append((old.upper(),new.upper(),place_holder[:-1]+'up$'))##place holderを少し変更する必要があった。\n",
    "    if old[0]==' ':\n",
    "        replacements2.append((old[0] + old[1].upper() + old[2:],new[0] + new[1].upper() + new[2:],place_holder[:-1]+'cap$'))##new[0] + new[1].upper() + new[2:]は本当は怪しいが。。  \n",
    "    else:\n",
    "        replacements2.append((old.capitalize(),new.capitalize(),place_holder[:-1]+'cap$'))\n",
    "len(replacements2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b74ca0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"replacements2\"リストの内容を確認\n",
    "with open(\"replacements_list_html.txt\", 'w', encoding='utf-8') as file:\n",
    "    for old,new,place_holder in replacements2:\n",
    "        file.write(f'{old},{new},{place_holder}\\n')\n",
    "#最終的な置換に用いる\"replacements2\"リストの長さ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af099f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements3=[]\n",
    "with open(\"replacements_list_html.txt\", 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.rstrip()##stripでは置換に最初の空白を反映できなくなってしまう。\n",
    "        j = line.split(',')\n",
    "        if len(j)==3:\n",
    "            old,new,place_holder=j[0],j[1],j[2]\n",
    "            replacements3.append((old,new,place_holder))\n",
    "#len(replacements3)  \n",
    "#replacements2とreplacements3は全く同一のリストである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82324670",
   "metadata": {},
   "outputs": [],
   "source": [
    "##置換に用いる関数。正規表現、C++など様々な形式の置換を試したが、pythonで'place holder'を用いる形式の置換が、最も処理が高速であった。(しかも大変シンプルでわかりやすい。)\n",
    "def safe_replace(text, replacements):\n",
    "    valid_replacements = {}\n",
    "    # 置換対象(old)を'place holder'に一時的に置換\n",
    "    for old, new, placeholder in replacements:\n",
    "        if old in text:\n",
    "            text = text.replace(old, placeholder)\n",
    "            valid_replacements[placeholder] = new# 後で置換後の文字列(new)に置換し直す必要がある'place holder'を辞書(valid_replacements)に記録しておく。\n",
    "    #'place holder'を置換後の文字列(new)に置換)\n",
    "    for placeholder, new in valid_replacements.items():\n",
    "        text = text.replace(placeholder, new)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3697d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## multiprocessingのための関数群　テキストを行数によって設定プロセス数(num_processes)に等分割して、それぞれのプロセスで並列に置換処理を実行してから、再度分割したテキストを結合する。\n",
    "\n",
    "import multiprocessing\n",
    "def process_segment(lines, replacements):\n",
    "    # 文字列のリストを結合してから置換処理を実行 linesには\\nが含まれていない状態の文字列群が格納されている。\n",
    "    segment = '\\n'.join(lines)\n",
    "    segment = safe_replace(segment, replacements)#ここでsafe_replace関数の実行\n",
    "    return segment\n",
    "\n",
    "def parallel_process(text, num_processes,replacements):\n",
    "    #テキストを行で分割\n",
    "    lines = text.split('\\n')\n",
    "    num_lines = len(lines)\n",
    "    lines_per_process = num_lines // num_processes\n",
    "\n",
    "    #各プロセスに割り当てる行のリストを決定\n",
    "    ranges = [(i * lines_per_process, (i + 1) * lines_per_process) for i in range(num_processes)]\n",
    "    ranges[-1] = (ranges[-1][0], num_lines)  #最後のプロセスが残り全てを処理\n",
    "\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        # 並列処理を実行\n",
    "        results = pool.starmap(process_segment, [(lines[start:end], replacements) for start, end in ranges])\n",
    "\n",
    "    # 結果を結合\n",
    "    return '\\n'.join(result for result in results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "482dfe71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<ruby>心<rt>kor</rt></ruby>an <ruby>谢<rt>dank</rt></ruby>on vin sin lin <ruby>麻<rt>lin</rt></ruby>o <ruby>思<rt>sin</rt></ruby>o <ruby>爱<rt>am</rt></ruby>as <ruby>群<rt>amas</rt></ruby>o <ruby>有<rt>hav</rt></ruby>is aviso  <ruby>高<rt>alt</rt></ruby>aj altajo'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"koran dankon vin sin lin lino sino amas amaso havis aviso  altaj altajo\"\n",
    "text=replace_esperanto_chars(text,esperanto_to_x)\n",
    "safe_replace(text,replacements3)\n",
    "# parallel_process(text*1,1, replacements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c32cc1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<ruby>谢<rt>dank</rt></ruby>on vin sin lin <ruby>麻<rt>lin</rt></ruby>o <ruby>思<rt>sin</rt></ruby>o <ruby>爱<rt>am</rt></ruby>as <ruby>群<rt>amas</rt></ruby>o <ruby>有<rt>hav</rt></ruby>is aviso  <ruby>高<rt>alt</rt></ruby>aj altajo'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# safe_replace(\"Membroj havas  Mi farigxis multaj taskoj  Mi amas vin multe necese movado sin senegal nova \",replacements3)\n",
    "# safe_replace(\"mondon mondonas amon amo amonj! amojn! mondon mondo vino vinon  vinojn vin vino nova \",replacements3)\n",
    "# safe_replace(\"sin sindoni sindonemo dion Dion amon amon dio dia Senegala senegal senegala senegalo\",replacements3)\n",
    "safe_replace(\"dankon vin sin lin lino sino amas amaso havis aviso  altaj altajo\",replacements3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a167267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_strings_in_text(text):\n",
    "    # 正規表現パターンを定義\n",
    "    pattern = re.compile(r'%%(.{1,50}?)%%')\n",
    "    matches = []\n",
    "    used_indices = set()\n",
    "\n",
    "    # 正規表現のマッチを見つける\n",
    "    for match in pattern.finditer(text):\n",
    "        start, end = match.span()\n",
    "        # 重複する%%を避けるためにインデックスをチェック\n",
    "        if start not in used_indices and end-2 not in used_indices:  # end-2 because of double %%\n",
    "            matches.append(match.group(1))\n",
    "            # インデックスを使用済みセットに追加\n",
    "            used_indices.update(range(start, end))\n",
    "    return matches\n",
    "def load_placeholders(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        placeholders = [line.strip() for line in file if line.strip()]\n",
    "    return placeholders\n",
    "def create_replacements(text, placeholders):\n",
    "    # テキストから%%で囲まれた部分を抽出\n",
    "    matches = find_strings_in_text(text)\n",
    "    replacements_list_for_intact_parts = []\n",
    "    # プレースホルダーとマッチを対応させる\n",
    "    for i, match in enumerate(matches):\n",
    "        if i < len(placeholders):\n",
    "            replacements_list_for_intact_parts.append([f\"%%{match}%%\", placeholders[i]])\n",
    "        else:\n",
    "            break  # プレースホルダーが足りなくなった場合は終了\n",
    "    return replacements_list_for_intact_parts\n",
    "# 使用例\n",
    "text = text = \"\"\"%%L. Zamenhof.%% Kongresaj paroladoj\n",
    "Kongresaj paroladoj\n",
    "%%Lazarj Markoviĉ Zamenhof%%\n",
    "\n",
    "Kongresaj paroladoj\n",
    "\n",
    "Serio Scio, №2\n",
    "\n",
    "La 1a eldono: %%Jekaterinburgo%% Ruslanda Esperantisto, 1995.\n",
    "La 2a eldono: %%Kaliningrado%%: Sezonoj; %%Kaunas: LEA, 2015.%%\n",
    "\"\"\"\n",
    "# プレースホルダーファイルから読み込む\n",
    "placeholders = load_placeholders('No.1000_9999.txt')\n",
    "# リストを作成\n",
    "replacements_list_for_intact_parts = create_replacements(text, placeholders)\n",
    "sorted_replacements_list_for_intact_parts = sorted(replacements_list_for_intact_parts, key=lambda x: len(x[0]), reverse=True)\n",
    "# 結果を表示\n",
    "# for item in sorted_replacements_list_for_intact_parts:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bdd131a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('例文2.txt','r') as g:\n",
    "    ll=g.read()\n",
    "text2=replace_esperanto_chars(ll,esperanto_to_x)\n",
    "\n",
    "replacements_list_for_intact_parts = create_replacements(text2, placeholders)\n",
    "sorted_replacements_list_for_intact_parts = sorted(replacements_list_for_intact_parts, key=lambda x: len(x[0]), reverse=True)\n",
    "for original, place_holder_ in sorted_replacements_list_for_intact_parts:\n",
    "    text2 = text2.replace(original, place_holder_)\n",
    "\n",
    "text3=safe_replace(text2, replacements3)\n",
    "\n",
    "for original, place_holder_ in sorted_replacements_list_for_intact_parts:\n",
    "    text3 = text3.replace(place_holder_, original.replace(\"%%\",\"\"))\n",
    "\n",
    "# text3=parallel_process(text2*1,1, replacements2)\n",
    "with open('出力2.html','w', encoding='utf-8') as h:\n",
    "    h.write(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7f9f51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['%%Lazarj Markovicx Zamenhof%%', '&%1001&%'],\n",
       " ['%%Aleksander Korjxenkov%%', '&%1005&%'],\n",
       " ['%%Aleksander Korjxenkov%%', '&%1007&%'],\n",
       " ['%%Aleksander Korjxenkov%%', '&%1010&%'],\n",
       " ['%%Aleksander Korjxenkov%%', '&%1011&%'],\n",
       " ['%%Aleksander Korjxenkov%%', '&%1012&%'],\n",
       " ['%%Aleksander Korjxenkov%%', '&%1013&%'],\n",
       " ['%%Kaunas: LEA, 2015.%%', '&%1004&%'],\n",
       " ['%%Jekaterinburgo%%', '&%1002&%'],\n",
       " ['%%Halina Gorecka%%', '&%1008&%'],\n",
       " ['%%Halina Gorecka%%', '&%1009&%'],\n",
       " ['%%L. Zamenhof.%%', '&%1000&%'],\n",
       " ['%%Kaliningrado%%', '&%1003&%'],\n",
       " ['%%Alla Dzjuba%%', '&%1006&%']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_replacements_list_for_intact_parts\n",
    "# text2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7713af7",
   "metadata": {},
   "source": [
    "以下是附录(以下はおまけ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2148aa18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3efd58e0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
